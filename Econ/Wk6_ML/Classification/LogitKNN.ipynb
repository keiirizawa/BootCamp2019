{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Logit and KKN\n",
    "### by [Richard W. Evans](https://sites.google.com/site/rickecon/), February 2019\n",
    "The code in this Jupyter notebook was written using Python 3.6. It uses data files `?`, and ? image files in the `images` folder in the same directory as this notebook. This data file and image file are stored along with the Jupyter notebook file `Logit_KNN_LDA.ipynb` in the public GitHub repository [https://github.com/rickecon/Notebooks/tree/master/Classification](https://github.com/rickecon/Notebooks/tree/master/Classification). For the code to run properly, you will either need to have access to the internet or you should have the data files in the same folder as the Jupyter notebook file. Otherwise, you will have to change the respective lines of the code that read in the data to reflect the location of that data. Much of this discussion follows the presentation in chapters 2, 3, and 4 of [JWHT17].\n",
    "\n",
    "For teaching and learning purposes, it is best to clear all cell output by selecting `Cell` then `All Output` then `Clear` from the menu across the top of this notebook. However, this notebook comes with all the cells' output displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quantitative Versus Qualitative Data\n",
    "The regression models of the Linear Regression notebook have quantitative variables as dependent variables. That is, the $y_i$ variable takes on a continuum of values. We use a different class of models to estimate the relationship of exogenous variables to *qualitative* or *categorical* endogenous or dependent variables.\n",
    "\n",
    "Examples of qualitative or categorical variables include:\n",
    "\n",
    "* Binary variables take on two values ($J=2$), most often 0 or 1. Examples: Male or female, dead or alive, accept or reject.\n",
    "* General categorical variables can take on more than two values ($J\\geq 2$). Examples: red, blue, or green; teenager, young adult, middle aged, senior.\n",
    "\n",
    "Note with general categorical variables that order and numerical distance do not matter. As an example let $FlowerColor_i=\\{red=1, blue=2,green=3\\}$ be a function of $neighborhood_i$, $season_i$, and $income_i$.\n",
    "\n",
    "$$ FlowerColor_i = \\beta_0 + \\beta_1 neighborhood_i + \\beta_2 season_i + \\beta_3 income_i + u_i $$\n",
    "\n",
    "We could mathematically estimate this regression model, but would that make sense? What would be wrong with a regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The classification setting\n",
    "Let $y_i$ be a qualitative dependent variable on $N$ observations with $i$ being the index of the observation, and let $x_{p,i}$ be the $i$th observation of the $p$th explanatory variable (independent variable) such that $X_i=\\{x_{1,i}, x_{2,i}, ... x_{P,i}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Classifier\n",
    "In this section, we will look at two models for binary (0 or 1) categorical dependent variables. We describe the first model--the linear probability (LP) model--for purely illustrative purposes. This is because the LP model has some serious shortcomings that make it almost strictly dominated by our second model in this section.\n",
    "\n",
    "The second model--the logistic regression (logit, binary classifier) model--will be the focus of this section. There is another variant of this model, the probit model. But the logistic model is the more flexible, more easily interpretable, and more commonly used of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The linear probability (LP) model\n",
    "One option in which a regression is barely acceptable for modeling a binary (categorical) dependent variable is the linear probability (LP) model. When the dependent variable has only two categories, it can be modeled as $y_i\\in\\{0,1\\}$ without loss of generality. Let the variable $z_i$ be interpreted as the probability that $y_i=1$ given the data $X_i$ and parameter values $\\theta=\\{\\beta_0,\\beta_1,...\\beta_P\\}$.\n",
    "\n",
    "$$ z_i = Pr(y_i=1|X_i,\\theta) = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... \\beta_P x_{P,i} + u_i $$\n",
    "\n",
    "The LP model can be a nice, easy, computationally convenient way to estimate the probability of outcome $y_i=1$. This could also be reinterpreted, without loss of generality, as the probability that $y_i=0$. This is equivalent to a redefinition of which outcome is defined as $y_i=1$.\n",
    "\n",
    "The main drawback of the LP model is that the predicted values of the probability that $y_i=1$ or $Pr(y_i=1|X_i,\\theta)$ can be greater than 1 and can be less than 0. It is for this reason that it is very difficult to publish any research based on an LP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The logistic (logit) regression classifier\n",
    "The logistic regression model is a binary dependent variable classifier that constrains its predicted values to be stricly between 0 and 1. This is done by using a sigmoid function as the model on the right-hand-side of the equation,\n",
    "\n",
    "$$ f(x) = \\frac{g(x)}{1 + g(x)} \\quad\\text{for}\\quad x\\in(-\\infty,\\infty) \\quad\\text{and}\\quad g(x)\\geq 0 \\quad\\forall x $$\n",
    "\n",
    "and has the following general shape.\n",
    "\n",
    "![sigmoid.png](attachment:sigmoid.png)\n",
    "\n",
    "The logistic function is the specific case of the sigmoid function in which $g(X)= e^{X\\beta} = e^{\\beta_0 + \\beta_1 x_{1,i} + ...\\beta_P x_{P,i}}$.\n",
    "\n",
    "$$ Pr(y_i=1|X_i,\\theta) = \\frac{e^{X_i\\beta}}{1 + e^{X_i\\beta}} = \\frac{e^{\\beta_0 + \\beta_1 x_{1,i} + ...\\beta_P x_{P,i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1,i} + ...\\beta_P x_{P,i}}} $$\n",
    "\n",
    "We could estimate the paramters $\\theta=\\{\\beta_0,\\beta_1,...\\beta_P\\}$ by GMM using nonlinear least squares or a more general set of moments to match. But maximum likelihood estimation is the most common method for estimating the parameters $\\theta$ because of its more robust statistical properties. Also, the distributional assumptions are built into the model, so they are not overly strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Nonlinear least squares estimation\n",
    "If we define $z_i = Pr(y_i=1|X_i,\\theta)$, then the error in the logistic regression is the following.\n",
    "\n",
    "$$ \\varepsilon_i = y_i - z_i $$\n",
    "\n",
    "The GMM specification of the nonlinear least squares method of estimating the parameter vector $\\theta$ would then be the following.\n",
    "\n",
    "$$ \\hat{\\theta}_{nlls} = \\theta:\\quad \\min_{\\theta} \\sum_{i=1}^N\\varepsilon_i^2 \\quad = \\quad \\min_{\\theta}\\sum_{i=1}^N\\bigl(y_i - z_i \\bigr)^2 \\quad = \\quad \\min_{\\theta} \\sum_{i=1}^N\\Bigl[y_i - Pr(y_i=1|X_i,\\theta)\\Bigr]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Maximum likelihood estimation\n",
    "We characterized the general likelihood function for a sample of data as the probability that the given sample $(y_i,X_i)$ came from the assumed distribution given parameter values $Pr(y_i=1|X_i,\\theta)$.\n",
    "\n",
    "$$ \\mathcal{L}(y_i,X_i|\\theta) = \\prod_{i:y_i=1}Pr(y_i=1|X_i,\\theta)\\prod_{i':y_{i'}=0}\\Bigl[1 - Pr(y_i=1|X_i,\\theta)\\Bigr] $$\n",
    "\n",
    "The intuition of this likelihood function is that you want the probability of the observations for which $y_i=1$ to be close to one $Pr(X)$, and you want the probability of the observations for which $y_i=0$ to also be close to one $1 - Pr(X)$.\n",
    "\n",
    "The log-likelihood function, which the MLE problem maximizes is the following.\n",
    "\n",
    "$$\\ln\\bigl[\\mathcal{L}(y_i,X_i|\\theta)\\bigr] = \\sum_{i:y_i=1}\\ln\\bigl[Pr(y_i=1|X_i,\\theta)\\bigr] + \\sum_{i':y_{i'}=0}\\ln\\biggl(\\Bigl[1 - Pr(y_i=1|X_i,\\theta)\\Bigr]\\biggr) $$\n",
    "\n",
    "The MLE problem for estimating $\\theta$ of the logistic regression model is, therefore, the following.\n",
    "\n",
    "$$ \\hat{\\theta}_{mle} = \\theta:\\quad \\max_{\\theta} \\ln\\bigl[\\mathcal{L}(y_i,X_i|\\theta)\\bigr] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Titanic example\n",
    "A good example of logistic regression comes from a number of sources. But I am adapting some code and commentary from [http://www.data-mania.com/blog/logistic-regression-example-in-python/](http://www.data-mania.com/blog/logistic-regression-example-in-python/). The research question is to use a famous Titanic passenger dataset to try to identify the characteristics that most predict whether you survived $y_i=1$ or died $y_i=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "url = ('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/' +\n",
    "      'master/titanic-train.csv')\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age',\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VARIABLE DESCRIPTIONS **\n",
    "\n",
    "Survived - Survival (0 = No; 1 = Yes)\n",
    "Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "Name - Name\n",
    "Sex - Sex\n",
    "Age - Age\n",
    "SibSp - Number of Siblings/Spouses Aboard\n",
    "Parch - Number of Parents/Children Aboard\n",
    "Ticket - Ticket Number\n",
    "Fare - Passenger Fare (British pound)\n",
    "Cabin - Cabin\n",
    "Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "**Checking that your target variable is binary**\n",
    "\n",
    "Since we are building a model to predict survival of passangers from the Titanic, our target is going to be \"Survived\" variable from the titanic dataframe. To make sure that it's a binary variable, let's use Seaborn's countplot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10a7fb630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='Survived', data=titanic, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for missing values**\n",
    "\n",
    "It's easy to check for missing values by calling the `isnull()` method, and the `sum()` method off of that, to return a tally of all the `True` values that are returned by the `isnull()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many observations are there in the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection and missing values**\n",
    "\n",
    "The variable `Cabin` is almost all missing values, so we should not include that in our analysis (although that would be an interesting variable to have). We can also probably exclude `Name`, `Ticket` (ticket number). But we will include all the other variables.\n",
    "\n",
    "* Survived - This variable is obviously relevant.\n",
    "* Pclass - Does a passenger's class on the boat affect their survivability?\n",
    "* Sex - Could a passenger's gender impact their survival rate?\n",
    "* Age - Does a person's age impact their survival rate?\n",
    "* SibSp - Does the number of relatives on the boat (that are siblings or a spouse) affect a person survivability? Probably.\n",
    "* Parch - Does the number of relatives on the boat (that are children or parents) affect a person survivability? Probably.\n",
    "* Fare - Does the fare a person paid effect his survivability? Maybe.\n",
    "* Embarked - Does a person's point of embarkation matter? It depends on how the boat was filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data = titanic.drop(['PassengerId','Name','Ticket','Cabin'], 1)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the dataframe reduced down to only relevant variables, but now we need to deal with the missing values in the age variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputing missing values**\n",
    "\n",
    "Let's look at how passenger age is related to their class as a passenger on the boat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    491\n",
      "1    216\n",
      "2    184\n",
      "Name: Pclass, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1997aef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(titanic_data['Pclass'].value_counts())\n",
    "sb.boxplot(x='Pclass', y='Age', data=titanic_data, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking roughly, we could say that the younger a passenger is, the more likely it is for them to be in 3rd class. The older a passenger is, the more likely it is for them to be in 1st class. So there is a loose relationship between these variables. So, let's write a function that approximates a passengers age, based on their class. From the box plot, it looks like the average age of 1st class passengers is about 37, 2nd class passengers is 29, and 3rd class pasengers is 24.\n",
    "\n",
    "So let's write a function that finds each null value in the `Age` variable, and for each null, checks the value of the Pclass and assigns an age value according to the average age of passengers in that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age         0\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "Embarked    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def age_approx(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 24\n",
    "    else:\n",
    "        return Age\n",
    "\n",
    "titanic_data['Age'] = titanic_data[['Age', 'Pclass']].apply(age_approx, axis=1)\n",
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 null values in the `Embarked` variable. We can drop those 2 records without loosing too much important information from our dataset, so we will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.dropna(inplace=True)\n",
    "titanic_data.isnull().sum()\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting categorical variables to a dummy indicators**\n",
    "\n",
    "The next thing we need to do is reformat our variables so that they work with the model. Specifically, we need to reformat the Sex and Embarked variables into numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male      577\n",
      "female    312\n",
      "Name: Sex, dtype: int64\n",
      "   male\n",
      "0     1\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     1\n"
     ]
    }
   ],
   "source": [
    "print(titanic_data['Sex'].value_counts())\n",
    "gender = pd.get_dummies(titanic_data['Sex'], drop_first=True)\n",
    "print(gender.head())\n",
    "# gender2 = pd.get_dummies(titanic_data['Sex'], drop_first=False)\n",
    "# gender2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q  S\n",
       "0  0  1\n",
       "1  0  0\n",
       "2  0  1\n",
       "3  0  1\n",
       "4  0  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(titanic_data['Embarked'].value_counts())\n",
    "embark_location = pd.get_dummies(titanic_data['Embarked'], drop_first=True)\n",
    "embark_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare\n",
       "0         0       3  22.0      1      0   7.2500\n",
       "1         1       1  38.0      1      0  71.2833\n",
       "2         1       3  26.0      0      0   7.9250\n",
       "3         1       1  35.0      1      0  53.1000\n",
       "4         0       3  35.0      0      0   8.0500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  male  Q  S\n",
       "0         0       3  22.0      1      0   7.2500     1  0  1\n",
       "1         1       1  38.0      1      0  71.2833     0  0  0\n",
       "2         1       3  26.0      0      0   7.9250     0  0  1\n",
       "3         1       1  35.0      1      0  53.1000     0  0  1\n",
       "4         0       3  35.0      0      0   8.0500     1  0  1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_dmy = pd.concat([titanic_data, gender, embark_location], axis=1)\n",
    "titanic_dmy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataset with all the variables in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for correlation vs. independence between variables**\n",
    "\n",
    "We can use `seaborn`'s `.heatmap` method to get a color-coded heatmap of the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a19ab3630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.heatmap(titanic_dmy.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is OK for the first row and first column to have correlation (dark red or dark blue). But we want to avoid heavy correlation among the independent variables (regressors). It looks like `Fare` and `Pclass` are highly negatively correlated, so let's just keep `Fare`. `Fare` is nice becaue it is more continuous than `Pclass`. Also, the embark locations of `Q` and `S` are highly negatively correlated, but that is to be expected because they are mutually exclusive. So we will keep both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age  SibSp  Parch     Fare  male  Q  S\n",
       "0         0  22.0      1      0   7.2500     1  0  1\n",
       "1         1  38.0      1      0  71.2833     0  0  0\n",
       "2         1  26.0      0      0   7.9250     0  0  1\n",
       "3         1  35.0      1      0  53.1000     0  0  1\n",
       "4         0  35.0      0      0   8.0500     1  0  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_dmy.drop(['Pclass'], axis=1, inplace=True)\n",
    "titanic_dmy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking that your dataset size is sufficient**\n",
    "\n",
    "We have 6 predictive features that remain. The rule of thumb is 50 records per regressor. So we need to have at least 350 records in this dataset. Let's check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 889 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      "Survived    889 non-null int64\n",
      "Age         889 non-null float64\n",
      "SibSp       889 non-null int64\n",
      "Parch       889 non-null int64\n",
      "Fare        889 non-null float64\n",
      "male        889 non-null uint8\n",
      "Q           889 non-null uint8\n",
      "S           889 non-null uint8\n",
      "dtypes: float64(2), int64(3), uint8(3)\n",
      "memory usage: 44.3 KB\n"
     ]
    }
   ],
   "source": [
    "titanic_dmy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to divide up the data into training data $(y_{trn,i}, X_{trn,i})$ and test data $(y_{tst,i}, X_{tst,i})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.  1.  0. ...  1.  0.  1.]\n",
      " [38.  1.  0. ...  0.  0.  0.]\n",
      " [26.  0.  0. ...  0.  0.  1.]\n",
      " ...\n",
      " [24.  1.  2. ...  0.  0.  1.]\n",
      " [26.  0.  0. ...  1.  0.  0.]\n",
      " [32.  0.  0. ...  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# X and y are numpy arrays, not a DataFrame and Series, respectively.\n",
    "\n",
    "X = titanic_dmy[['Age', 'SibSp', 'Parch', 'Fare',\n",
    "                 'male', 'Q', 'S']].values\n",
    "y = titanic_dmy['Survived'].values\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function train_test_split is from sklearn.cross_validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .4,\n",
    "                                                    random_state=25)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to let scikit-learn run the maximum likelihood estimation of our multiple logistic regression classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.592903]\n",
      "[[-0.02558109 -0.31831145 -0.20176942  0.02056714 -2.40322119 -0.47723787\n",
      "  -0.37706721]]\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)\n",
    "print(LogReg.intercept_)\n",
    "print(LogReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[180,  32],\n",
       "       [ 48,  96]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix tells us Type I and Type II errors. The results from the confusion matrix are telling us that 180 and 48 are the number of correct predictions. 96 and 32 are the number of incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.85      0.82       212\n",
      "          1       0.75      0.67      0.71       144\n",
      "\n",
      "avg / total       0.77      0.78      0.77       356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02558109, -0.31831145, -0.20176942,  0.02056714, -2.40322119,\n",
       "        -0.47723787, -0.37706721]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.592903])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels.api` package also has a `logit` model that can be run in a similar way to how we ran the linear regression model in the [Linear Regression]() notebook. It is interesting to note the difference between the `scikit-learn` logit model and the `statsmodel` one. `scikit-learn` is focused on prediction accuracy and makes it a little more difficult to access the estimated parameters. The `statsmodel` output is more focused on the estimated parameters and model fit, and it is more work to test the predictive power. For this reason, the studying logistic regression provides a nice bridge into machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning does not care about standard errors. However, we have every information we need to compute standard errors. We use a package in api to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.475647\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>   889</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   881</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 05 Aug 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.2850</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>09:19:12</td>     <th>  Log-Likelihood:    </th> <td> -422.85</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -591.41</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>7.044e-69</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.9208</td> <td>    0.331</td> <td>    5.811</td> <td> 0.000</td> <td>    1.273</td> <td>    2.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.0183</td> <td>    0.007</td> <td>   -2.572</td> <td> 0.010</td> <td>   -0.032</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.3929</td> <td>    0.104</td> <td>   -3.790</td> <td> 0.000</td> <td>   -0.596</td> <td>   -0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.2213</td> <td>    0.114</td> <td>   -1.943</td> <td> 0.052</td> <td>   -0.445</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0148</td> <td>    0.003</td> <td>    5.179</td> <td> 0.000</td> <td>    0.009</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -2.6362</td> <td>    0.190</td> <td>  -13.865</td> <td> 0.000</td> <td>   -3.009</td> <td>   -2.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.7175</td> <td>    0.362</td> <td>   -1.982</td> <td> 0.048</td> <td>   -1.427</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.5003</td> <td>    0.226</td> <td>   -2.210</td> <td> 0.027</td> <td>   -0.944</td> <td>   -0.057</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  889\n",
       "Model:                          Logit   Df Residuals:                      881\n",
       "Method:                           MLE   Df Model:                            7\n",
       "Date:                Mon, 05 Aug 2019   Pseudo R-squ.:                  0.2850\n",
       "Time:                        09:19:12   Log-Likelihood:                -422.85\n",
       "converged:                       True   LL-Null:                       -591.41\n",
       "                                        LLR p-value:                 7.044e-69\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.9208      0.331      5.811      0.000       1.273       2.569\n",
       "x1            -0.0183      0.007     -2.572      0.010      -0.032      -0.004\n",
       "x2            -0.3929      0.104     -3.790      0.000      -0.596      -0.190\n",
       "x3            -0.2213      0.114     -1.943      0.052      -0.445       0.002\n",
       "x4             0.0148      0.003      5.179      0.000       0.009       0.020\n",
       "x5            -2.6362      0.190    -13.865      0.000      -3.009      -2.264\n",
       "x6            -0.7175      0.362     -1.982      0.048      -1.427      -0.008\n",
       "x7            -0.5003      0.226     -2.210      0.027      -0.944      -0.057\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the numpy array\n",
    "num_obs = X.shape[0]\n",
    "const_vec = np.ones(num_obs).reshape((num_obs, 1))\n",
    "XplusConst = np.hstack((const_vec, X))\n",
    "\n",
    "LogitModel = sm.Logit(y, XplusConst)\n",
    "LogitReg_sm = LogitModel.fit()\n",
    "LogitReg_sm.summary()\n",
    "# Remember y and X variables are\n",
    "# y  = Survived (1, 0)\n",
    "# x1 = Age\n",
    "# x2 = SibSp\n",
    "# x3 = Parch\n",
    "# x4 = Fare\n",
    "# x5 = male\n",
    "# x6 = Q\n",
    "# x7 = S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Interpreting coefficients (log odds ratio)\n",
    "The odds ratio in the logistic model is provides a nice way to interpret logit model coefficients. Let $z\\equiv X^T\\beta = \\beta_0 + \\beta_1 x_{1,i} + ...\\beta_K x_{K,i}$. The logistic model is stated by the probability that the binary categorical dependent variable equals one $y_i=1$.\n",
    "\\begin{equation}\n",
    "  P(y_i=1|X,\\theta) = \\frac{e^z}{1 + e^z}\n",
    "\\end{equation}\n",
    "Given this equation, we know that the probability of the dependent variable being zero $y_i=0$ is just one minus the probability above.\n",
    "\\begin{equation}\n",
    "  P(y_i=0|X,\\theta) = 1 - P(y_i=1|X,\\theta) = 1 - \\frac{e^z}{1 + e^z} = \\frac{1}{1 + e^z}\n",
    "\\end{equation}\n",
    "\n",
    "The odds ratio is a common way of expressing the probability of an event versus all other events. For example, if the probability of your favorite team winning a game is $P(win)=0.8$, then we know that the probability of your favorite team losing that game is $P(lose)=1-P(win)=0.2$. The odds ratio is the ratio of these two probabilities.\n",
    "\\begin{equation}\n",
    "  \\frac{P(win)}{P(lose)} = \\frac{P(win)}{1 - P(win)} = \\frac{0.8}{0.2} = \\frac{4}{1} \\quad\\text{or}\\quad 4\n",
    "\\end{equation}\n",
    "The odds ratio tells you that the probability of your team winning is four times as likely as your team losing. A gambler would say that your odds are 4-to-1. Another way of saying it is that your team will win four out of five times and will lose 1 out of five times.\n",
    "\n",
    "In the logistic model, the odds ratio reduces the problem nicely.\n",
    "\\begin{equation}\n",
    "  \\frac{P(y_i=1|X,\\theta)}{1 - P(y_i=1|X,\\theta)} = \\frac{\\frac{e^z}{1 + e^z}}{\\frac{1}{1 + e^z}} = e^z\n",
    "\\end{equation}\n",
    "If we take the log of both sides, we see that the log odds ratio is equal to the linear predictor $z\\equiv X^T\\beta = \\beta_0 + \\beta_1 x_{1,i} + ...\\beta_K x_{K,i}$.\n",
    "\\begin{equation}\n",
    "  \\ln\\left(\\frac{P(y_i=1|X,\\theta)}{1 - P(y_i=1|X,\\theta)}\\right) = z = \\beta_0 + \\beta_1 x_{1,i} + ...\\beta_K x_{K,i}\n",
    "\\end{equation}\n",
    "\n",
    "So the interpretation of the coeficients $\\beta_k$ is that a one-unit increase of the variable $x_{k,i}$ increases the odds ratio or the odds of $y_i=1$ by $\\beta_{k,i}$ percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-nearest Neighbors (KNN) Classifier\n",
    "The $K$-nearest Neighbors (KNN) classifier is a nonparametric model, based on the value of the parameter $K$ that predicts the category of a given observation. It is often referred to as an unsupervised learning method because it, in some sense, requires no model. The KNN method simply uses the data in the neighborhood of each datapoint to predict type. Let $J\\geq 2$ be the number of possible categories such that $y_i\\in\\{1,2,...J\\}$. Assume a special case in which we have data on two variables $X_i=(x_{1,i}, x_{2,i})$ for each observation. This example is easy to visualize (2 dimensional data).\n",
    "\n",
    "The KNN classifier finds the $K$ points in the data that are closest to $X_0$ and calculates the conditional probability for each category $j$ in that set of $K$ points. Define the set of observations $i$ in that set of $K$ points closest to $X_0$ as $\\mathcal{N}_0$. Euclidean distance is often used,\n",
    "\n",
    "\\begin{equation}\n",
    "  d(x,x') \\equiv \\sqrt{(x_1-x_1')^2 + (x_2-x_2')^2 + ... (x_N-x_N')^2}\n",
    "\\end{equation}\n",
    "\n",
    "although other distance metrics can be used and might even be more suitable. Examples include the Manhattan, Chebyshev, and Hamming distance. The KNN model is specified as follows.\n",
    "\n",
    "$$ Pr(Y=j|X=X_0) = \\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}I\\left(y_i=j\\right) $$\n",
    "\n",
    "Notice that the left-hand-side of this model has similar structure to the logistic regression model. The KNN classifier applies Bayes rule and classifies the test observation $X_0$ to the class with the largest probability.\n",
    "\n",
    "The Figure 2.14 from [JWHT17] illustrates a KNN classifier with $K=3$ for six blue and six orange observations. The left pannel highlights the $K=3$ nearest observations to the observation marked \"x\". Of those three nearest neighbors, 2/3 are blue and 1/3 are orange. So Bayes rule classifies the observation marked \"x\" as blue.\n",
    "\n",
    "![Fig2_14.png](images/Fig2_14.png)\n",
    "\n",
    "The panel on the right in Figure 2.14 shows the decision boundary for each classification. This boundary is the point where the two categories have the same probability Pr(Y=j|X) = 1/2.\n",
    "\n",
    "Figure 2.15 below shows a larger set of simulated binary dependent variable data for KNN classifier $K=10$. Notice that the black line decision boundary is very close to the optimal dashed line Bayes classifier (if you knew the true data generating process). We show this to demonstrate how well and how flexibly a KNN classifier can perform.\n",
    "\n",
    "![Fig2_15.png](images/Fig2_15.png)\n",
    "\n",
    "Figure 2.16 below presents two extremes in KNN classifiers. The left panel is the $K=1$ KNN classifier. This KNN simply assigns the value of the closest neighbor to the data. Notice how rough the decision boundaries are. This KNN classifier is probably overfitting the data. That is, it is likely capturing too much noise to be good at predicting test data.\n",
    "\n",
    "![Fig2_16.png](images/Fig2_16.png)\n",
    "\n",
    "The right panel of Figure 2.16 shows a KNN classifier with $K=100$. This classifier barely has any contour at all. This classifier is likely underfitting the data or ignoring information. This can be seen by comparing the black KNN decision boundary to the dashed optimal Bayes classifier decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Examples: Iris and Digits\n",
    "[TODO] Include Iris example from [https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/) and digits data example from [http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "[0 1 2 ... 8 9 8]\n",
      "KNN score: 0.961111\n",
      "LogisticRegression score: 0.938889\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "print(X_digits)\n",
    "y_digits = digits.target\n",
    "print(y_digits)\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[:int(.9 * n_samples)]\n",
    "y_train = y_digits[:int(.9 * n_samples)]\n",
    "X_test = X_digits[int(.9 * n_samples):]\n",
    "y_test = y_digits[int(.9 * n_samples):]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Discriminant Analysis (LDA)\n",
    "[TODO] Put linear discriminant analysis (LDA) here. See Section 4.4 of [JWHT17]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multinomial Logit\n",
    "The multinomial logit model is a natural extension of the logit model. In contrast to the logit model in which the dependent variable has only two categories, the multinomial logit model accomodates $J\\geq2$ categories in the dependent variable. Let $\\eta_j$ be the linear predictor for the $j$th category.\n",
    "$$ \\eta_j\\equiv \\beta_{j,0} + \\beta_{j,1}x_{1,i} + ...\\beta_{j,K}x_{K,i} \\quad\\forall y_i = j $$\n",
    "\n",
    "The multinomial logit model gives the probability of $y_i=j$ relative to some reference category $J$ that is left out.\n",
    "\\begin{equation}\n",
    "  Pr(y_i=j|X,\\theta) = \\frac{e^{\\eta_j}}{1 + \\sum_v^{J-1}e^{\\eta_v}} \\quad\\text{for}\\quad 1\\leq j\\leq J-1\n",
    "\\end{equation}\n",
    "\n",
    "Once the $J-1$ sets of coefficients are estimated, the final $J$th set of coefficients are a residual based on the following expression.\n",
    "\\begin{equation}\n",
    "  Pr(y_i=J|X,\\theta) = \\frac{1}{1 + \\sum_v^{J-1}e^{\\eta_v}}\n",
    "\\end{equation}\n",
    "\n",
    "The analogous log odds ratio interpretation applies to the multinomial logit model.\n",
    "\\begin{equation}\n",
    "  \\ln\\left(\\frac{Pr(y_i=j|X,\\theta)}{Pr(y_i=J|X,\\theta)}\\right) = \\eta_j = \\beta_{j,0} + \\beta_{j,1}x_{1,i} + ...\\beta_{j,K}x_{K,i} \\quad\\text{for}\\quad 1\\leq j \\leq J-1\n",
    "\\end{equation}\n",
    "This is the odds ratio of $y_i=j$ relative to $y_i=J$. The interpretation of the $\\beta_{j,k}$ coefficient is the predicted percentage change in the log odds ratio of $y_i=j$ to $y_i=J$ from a one-unit increase in variable $x_{k,i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([50, 50, 50]))\n",
      "(150, 4)\n",
      "[0 0]\n",
      "[[9.81802952e-01 1.81970335e-02 1.43470785e-08]\n",
      " [9.71801252e-01 2.81987186e-02 2.97708794e-08]]\n",
      "0.9733333333333334\n",
      "[  9.88068195   2.21932072 -12.10000266]\n",
      "[[-0.42332359  0.96165008 -2.5193638  -1.08617659]\n",
      " [ 0.53399144 -0.31779047 -0.20533556 -0.93955173]\n",
      " [-0.11066785 -0.64385961  2.72469936  2.02572832]]\n"
     ]
    }
   ],
   "source": [
    "# This code comes from the scikit-learn example for multinomial logit\n",
    "# at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# The y variable is J=3 categories of iris (0=Setosa, 1=Versicolour, 2=Virginica)\n",
    "# The four columns of X represent Sepal Length, Sepal Width, Petal Length and Petal Width\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(np.unique(y, return_counts=True))\n",
    "print(X.shape)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X, y)\n",
    "print(clf.predict(X[:2, :]))\n",
    "print(clf.predict_proba(X[:2, :]))\n",
    "print(clf.score(X, y))\n",
    "\n",
    "print(clf.intercept_)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "* [JWHT17] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. *An Introduction to Statistical Learning with Applications in R*, Springer Texts in Statistics, Springer, 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
